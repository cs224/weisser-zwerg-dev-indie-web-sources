---
layout: "layouts/post-with-toc.njk"
title: "The Normal Metaphysics and Epistemology"
description: "Going beyond global skepticism with Holistic Structural Realism: The Co-evolution of Knowing (Epistemology) and Reality (Metaphysics)."
creationdate: 2025-09-08
date: 2025-09-08
keywords: structural realism, epistemology and metaphysics, model-based science, predictivism, theory change

tags: ['post']
# eleventyExcludeFromCollections: true
---

## Rationale

[Karl Popper](https://en.wikipedia.org/wiki/Karl_Popper) begins his short essay "The Myth of the Framework", the second essay in his 1994 book of the same name, [*The Myth of the Framework: In Defence of Science and Rationality*](https://en.wikipedia.org/wiki/The_Myth_of_the_Framework) (only 30 pages long and well worth reading), with the following words:

> One of the more disturbing aspects of the intellectual life of our time is the way in which *irrationalism* is so widely advocated, and the way in which irrationalist doctrines are taken for granted.
> One of the components of modern *irrationalism* is *relativism* (the doctrine that truth is relative to our intellectual background, which is supposed to determine somehow the framework within which we are able to think: that truth may change from one framework to another), and,
> in particular, the doctrine of the impossibility of mutual understanding between different cultures, generations, or historical periods - even within science, even within physics.
> In this paper I discuss the problem of *relativism*.
> It is my claim that behind it lies what I call "The Myth of the Framework".

Much of modern philosophy, from Kant's transcendental idealism through Hegel's absolute idealism and many post Kantian heirs, has encouraged the habit of treating thought systems as self contained "frameworks".
[Ayn Rand](https://en.wikipedia.org/wiki/Ayn_Rand) considered Immanuel Kant her philosophical opposite and goes so far as to call Kant "the most evil man in human history".
Karl Popper also criticizes German idealist philosophers such as Kant and Hegel, along with "a long chain of post-Kantian, that is, post-critical or post-rationalist philosophers - mainly German - who upheld the myth of the framework" and helped spread irrationalism and relativism.

> I will discuss, and argue against, a myth: a false story that is widely accepted, especially in Germany.
> From there it invaded America, where it became almost all-pervasive among intellectuals, and where it forms the background of some of the most flourishing schools of philosophy.

In "The Myth of the Framework", Popper argues that disagreement need not be trapped within perspectives and that rational, fruitful discussion is possible even when participants do not share a common set of basic assumptions or a common cultural background.
Popper's "The Myth of the Framework" is both an argument for a rational, objective reality in which truth can be discerned and a defense of the possibility that people from any background can have fruitful discussions.

This blog post takes up the same topic by proposing a *Normal Metaphysics and Epistemology*: a thin realist jump after which the scientific method carries you forward.
This *Normal Metaphysics and Epistemology* aims to provide the fastest ramp into testable work and adopts only as much philosophical, that is, not testable, epistemology and metaphysics as needed to get rational learning of objective reality off the ground.
I further argue that, grounded in that objective reality and using rational thought, communication with other humans is possible even when they come from very different cultural or educational backgrounds.

The pre-jump world is the world of philosophers arguing with philosophers, without any objective way to identify who is right.
Although these activities have some entertainment value, they do not contribute to a deeper understanding of the world.
Personally, I avoid spending even one microsecond thinking about undecidable problems, as I consider it a complete waste of time[^mentalmasturbation].

The *Normal Metaphysics and Epistemology* keeps the pre-jump thin and selects the small set that minimizes regret and maximizes learning speed.
I think of this as *engineering your hinges*, for example, making just enough commitment in the philosophical territory as a practical premise that licenses measurement, modeling, and testing.
It is just enough to run the scientific method.

The post-jump world is the world of objective reality that is discernible through the scientific method.
By judging theories through calibration, robustness, and out of sample performance, rival frameworks can be compared by their grip on stable structures rather than by rhetoric.
The aim is simple: to shine the light of objective truth into subjective darkness, to keep what keeps working, to retire what fails, and to show how knowing and reality co-evolve when we let the world referee our disputes[^realityhurts].

## The Normal Metaphysics and Epistemology

*Global skepticism* is the family of views that questions whether we can know anything at all about the external world.
It is true that no chain of reasoning and no sensory experiment can prove that anything exists beyond the present contents of your own consciousness.
A certain amount of philosophical metaphysics is necessary to move beyond *global skepticism*.
As a tribute to Steven Reiss's idea of *The Normal Personality*[^stevenreissnormal], I call this stance *The Normal Metaphysics and Epistemology*.

My position begins by candidly acknowledging a logical gap: we cannot demonstrate from within pure thought that a mind independent world exists, yet we commit to it anyway.
You perform a *jump*: you voluntarily step over the logical gap and land in reality, adopting as basic the existence of a mind-independent world governed by laws and populated by other minds.
Nothing forces this jump, yet refusing it would trap you in radical solipsism, where even the ideas of evidence, correction, or dialogue disappear.
Once you make that leap, you can treat biological evolution as the engine that equips creatures with broadly truth-tracking senses and cognitive habits, since a chronic mismatch with reality is lethal.
Human communities then refine those roughly accurate representations through continual evidence sharing and Bayesian style updating, so knowledge progresses even though individual perception is fallible.
A very similar trajectory appears in evolutionary epistemology and in related approaches such as Bayesian epistemology, which argue that variation-and-selection among mental models link survival to approximate truth.

This outlook combines an honest confession that reason alone cannot bridge the gap with a pragmatic confidence that, after the jump, evolutionary reliability and communal Bayesian updating give us a steadily improving grip on the reality we first embraced on trust.
From here on **you can ignore undecidable philosophical debates**, and let the scientific method carry you forward.

### Normal

I first learned about Karl Popper's essay "The Myth of the Framework" from David Deutsch, who mentioned it during his conversation on the Strange Loop podcast episode [David Deutsch: AGI, the origins of quantum computing, and the future of humanity](https://www.youtube.com/watch?v=IVA2bK9qjzE)[^daviddeutschtranscript].
In that episode, he also said:

> People are valuable because they are different. Everybody is *unfathomably* different from everyone else.

A similar sentiment appears in [What Universal Human Experiences Are You Missing Without Realizing It?](https://slatestarcodex.com/2014/03/17/what-universal-human-experiences-are-you-missing-without-realizing-it):

> Remember Galton's experiments on visual imagination? Some people just don't have it. And they never figured it out. They assumed no one had it, and when people talked about being able to picture objects in their minds, they were speaking metaphorically.
>
> And the people who did have good visual imaginations didn't catch them. The people without imaginations mastered this "metaphorical way of talking" so well that they passed for normal.
> No one figured it out until Galton sat everyone down together and said "Hey, can we be really really clear about exactly how literal we're being here?" and everyone realized they were describing different experiences.

It is common wisdom that people are different. I used to think that, at a zeroth order approximation, people are like me and only the higher order differences vary.
I noticed how different people really are once I became a manager responsible for other people's work.
When you are only a colleague, people simply do things differently and you leave it at that.
Once you are responsible for someone else's work, you try to find out where things went wrong, and in those conversations you start to see how different people really are.
They are different even at the zeroth order approximation, as Galton's experiments showed.

That being said, most of us are "normal" enough in Steven Reiss's The *Normal Personality* sense to have broadly truth tracking senses and truth tracking cognitive habits, since a chronic mismatch with reality is lethal.
In that way we are able to synchronize and communicate through our shared experience of reality.

## The Scientific Method

I said earlier that I consider it a complete waste of time to discuss pre-jump undecidable metaphysical and epistemological problems, but I want to say a few words about the scientific method.

### The Primacy of Concepts and Learning via Play

Many proponents of the myth of the framework focus strongly on language, like the French thinkers of "deconstruction", who largely fed the so-called "post-modern" turn from the 1990s onwards:
Derrida for the deconstruction of discourses, Foucault for the deconstruction of powers, Lyotard for the deconstruction of the notion of truth, etc.
My critique of these post-Kantian, that is, post-critical or post-rationalist positions begins already here.
I would argue that *concepts*[^andrickbegriffshandwerker] are primary and that language is only one way, and perhaps not even a good way, to communicate them.

**So what are concepts?** I argue that we discover *concepts* through play, like a child in a sandbox with sand and water.
There is a joke: What is the difference between a child and an adult man? The price of the toy.
In the same way, the experimental equipment that physicists use to play with the world is a very expensive version of the sandbox and water.

Personally, the wave particle duality analogy used to explain photons or electrons cost me several years of my life.
In retrospect I would say a photon is a photon is a photon, and you find out what a photon is by playing with it through physics experiments.
In that way your understanding of a photon is shaped.
No language involved or needed!

> You do not have to run those experiments yourself.
> It is enough to read about them and to be able to repeat them if you really want or need to.
> When reading, language is obviously involved, but in an inessential way.

Higher order *concepts* are abstract. **So what does abstract mean?** My short definition of abstract and abstraction is *seeing commonalities where others may not*.
A simple example is the natural numbers. While the numbers one, two, three, and so on do not exist as real objects, there is a commonality among three pieces of cake, three cars, three fingers, or three ideas.
That commonality is the abstract thing.

Most, if not all, abstractions can be and are discovered through physics, meaning through interaction with the real world.
Another nice example of an abstract idea that I like is the *concept* of [hysteresis](https://en.wikipedia.org/wiki/Hysteresis), which comes from magnetism and means that the forward process can be quite different from the backward process.
Many processes work the same in both directions, for example you get out of a situation in the same way you got into it.
But some processes behave differently.
A former colleague called the *hysteresis* idea *swimming trunks*, because they get wet quickly and take a long time to dry.
Once you have a *concept* like *hysteresis* or *swimming trunks* you can transfer it to other domains like trust.
Trust behaves like *swimming trunks* in the sense that trust  is quickly destroyed but takes a long time to build up.

**What are these concepts good for?** *Concepts* alone are not enough.
The line "All science is either physics or stamp collecting" is widely attributed to [Ernest Rutherford](https://en.wikipedia.org/wiki/Ernest_Rutherford).
I take it to mean that stamp collecting (descriptive/cataloging) is amassing and sorting facts, specimens, or categories, e.g. categorizing for categorizing sake.
Physics (explanatory/predictive), on the other hand, builds quantitative models that explain those facts and make testable predictions.

In order to build time dependent models to then derive testable predictions we need **operations** that tell us how to combine *concepts*.
This idea of *concepts* plus *operations* is essentially the same as the idea of **algebraic structures** in abstract algebra.
An algebraic structure is a *set* with one or more *operations* that satisfy certain axioms (e.g. [group](https://en.wikipedia.org/wiki/Group_(mathematics)), [ring](https://en.wikipedia.org/wiki/Ring_(mathematics)), [field](https://en.wikipedia.org/wiki/Field_(mathematics)), [vector-space](https://en.wikipedia.org/wiki/Vector_space), ...).
These *algebraic structures* are themselves *concepts* in the sense that they have a prove once, use everywhere character, e.g.,
the cancellation law holds in every group;
in any field, every nonzero element is invertible, so linear equations over that field can be solved using the same theorems;
in any vector space, every spanning set contains a basis and any two bases have the same size (dimension).
Homomorphisms and isomorphisms preserve the *operations*, which lets you transfer results between different concrete examples of the same abstract structure.

So the overall flow is: **play** leads to **concepts** and **operations**, which give **models**, which lead to **testable predictions**.

### Concepts as Archeology

Another metaphor I like to use is "*concepts* as artifacts unearthed from the sand of noisy data" in a similar way as an archeologist would unearth artifacts from the sand of a dig site.

Imagine a dig site spread across a desert of observations.
Most of what you see is sand: fluctuating backgrounds, measurement error, context you don't yet understand.
Somewhere beneath are artifacts: stable patterns that belong to the world, not to your whim.
A *concept*, in this picture, is an artifact-like invariant you expose, clean, place, and eventually work with inside a larger assemblage so that you can anticipate what you will find next.
The proof that you have discovered something real is pragmatic: your reconstruction lets you predict where to dig tomorrow and what you'll likely uncover when you get there.

Excavation begins when you define a unit and make your first cut.
Operationalizing a variable is like choosing your trench boundaries and layer thickness.
Your "cutoff to noise" in this metaphor is the mesh size of your sieve.
You pick it to keep the sand from overwhelming you, knowing full well you can swap in a finer or coarser mesh and see whether the putative artifact holds together.
Because your criterion for success is prediction out of sample, a bad mesh won't doom you;
it will simply show up later as fragile generalization.
In that sense, the cutoff is a non-issue; it's a tool setting.
If the piece falls apart when you try to reassemble it elsewhere, you re-sieve and try again.

Stratigraphy matters because the same object means different things in different layers.
Data have layers too - time periods, instruments, populations, contexts.
A *concept* that looks solid in one stratum can be a mirage in another.
Dating in archaeology has its analogue in calibration and external validation.

Cleaning follows. An artifact fresh from the ground is caked with accretions.
Denoising, filtering, and imputation are the scientist's brushes and baths.
There is always a danger of over-cleaning - polishing away genuine texture along with grime.
The only protection is reversibility and auditability: record what you did, keep a raw layer, and check whether the "clean" version actually improves out-of-sample forecasts.
If not, you've scrubbed too hard.

Classification and typology come next.
Archaeologists don't stop at a single shard; they sort pieces into types and families.
Likewise, once you think you've unearthed a *concept*, you look for its kin: do related datasets support variants of the same pattern, do parameters line up, does the thing recur at different scales?
When the typology stabilizes across independent digs - i.e., across unrelated datasets and instruments - you have early evidence of an invariant rather than a coincidence.

An isolated artifact is interesting; an assemblage that works is science.
Theory supplies *operations* that make *concepts* composable ("verknüpfen") - ways of composing, transforming, and inferring new ones.
In archaeology that's the moment you reconstruct the mechanism: how the pieces fit, what goes where, what follows if one element is present.
In mathematics you'd ask for closure, associativity, invariants.
In science you ask whether your *operations* respect the symmetries the world seems to enforce.
The crucial test is dynamic: run the *operations* forward and see whether they land you in a state the world itself will later occupy.
If the predicted future actually shows-up in reality, your *operations* track real structure; if not, you revise either the parts, the joins, or the grammar of combination.

The metaphor also explains failure modes without moralizing.
Pareidolia is when the mind sees a face in random swirls; in research it's overfitting.
Treasure hunting is when you cherry-pick shiny pieces; in research it's p-hacking.
Site contamination maps to dataset leakage.
None of these require talk of politics or power to diagnose; your own standard - "does the reconstruction predict what we later find?" - exposes them naturally.

This picture of science as carving *concepts* from observation, combining them with well defined *operations*, testing by prediction, and keeping only what keeps paying rent will ground you in the real world and help you both stand on a firm, rational, objective footing and
avoid falling prey to *irrationalism* and its derivative *relativism*.

### Digital-Twin

The idea that most of us are "normal" enough, in Steven Reiss's *The Normal Personality* sense, to have senses and thinking habits that broadly track the truth can also be understood by analogy with robots.
A robot keeps an internal *digital-twin* of objects in the real world. The *digital-twin* may not be perfect, but it is good enough, or even quite good, because otherwise the robot would not function well in the real world.

So the revised overall flow is: **play** leads to **digital-twin** of the real world from which we derive **concepts** and **operations**, which give **models**, which lead to **testable predictions**.

### Surprise: A Warning Signal that your Models are Wrong

A surprise is what your model assigns a very low probability to, yet it happens.
The more often that happens (frequency) and the more improbable those outcomes were (severity), the stronger the signal that your model is mis-specified, miscalibrated, or stale.

In my view theories are *operations* over *concepts* that must pay rent in prediction.
Surprise is the most compact early-warning signal that the *operations* aren't tracking the structure I thought they were.
Occasional surprises are expected - even good, because they teach.
But *frequent* or *high-severity* surprises mean you're extracting the wrong artifact from the sand (bad *concept*), using the wrong sieve (bad noise model), or digging in the wrong layer (shift/regime).
Surprise isn't an embarrassment; it's a diagnostic telling you to revise the model so tomorrow's world looks less "impossible" under today's predictions.

### Ethics: Raw Curiosity

Post-modern constructivist thinkers, a special kind of post-Kantian, that is, post-critical or post-rationalist philosophers, focus heavily on political power structures and very little on truth. They ask questions like:

* Who set the threshold, and why that one?
* Which data are missing?
* Which interests shaped the target/category?

Steven Reiss lists sixteen basic desires in his book *The Normal Personality*. One of them is *Curiosity*:

> Curiosity: the desire to gain knowledge and to understand how things work; it drives learning and exploration for their own sake.

Raw curiosity is the engine of real science, and reality decides through prediction.
The standard "does the reconstruction predict what we later find?" exposes bad science naturally.
If you use proper scoring, calibration, and rigorous out of sample tests, you have the debugging tools to correct your worldview within a truth seeking framework, without any need for politics.

### No Science Without Prediction

One of my teachers often said that any field that needs the word science in its name is not a science. This saying does not translate well into English, but in German it neatly picks out the hard sciences, such as physics and chemistry, and dismisses the soft sciences.

My focus on predictive success means that science without prediction is not science.
Without the full control loop from play to testable predictions, we cannot determine the quality of our approximations to truth.
The full control loop is essential!

> **Play** leads to a **digital twin** of the real world. From it we derive **concepts** and **operations**, which produce **models**, which lead to **testable predictions**.

The French philosopher and literary critic [Roland Barthes](https://en.wikipedia.org/wiki/Roland_Barthes) once wrote that a myth is at first only a kind of speech or a text, but one that twists reality, recasts our perception of it, and thus lies.
In his view, myths endure because their reinterpretation of reality is seductively simple.

In my view, any field that does not rely on experiment and does not use the full control loop is engaged in myth making rather than truth seeking.
I would suggest using another name for such fields of inquiry and reserving the term science for the hard sciences that use the full control loop.

#### Ambition of a Full Control Loop

Young sciences may not yet be able to use the full control loop, but the ambition to end up with one needs to be there to deserve the name science.

Early versions of a theory may not look like a full map of the territory that yields a model like the planetary motion around the sun, from which you can calculate detailed positions many years into the future.
They may look more like the "route based" maps used in ancient China, which combined small maps with text about roads, passes, rivers, and notable landmarks so a newcomer could travel through a region.
But over time, a young science will mature, and a full map of the territory may be developed.

#### Weak Coupling

Some aspects of the world are only weakly coupled to the rest of it, yet they can still produce effects of interest.
This means that in short term observations you may think you are seeing only noise, but over longer periods a real impact becomes observable.
Public policies are one example.
At first, one policy may look no better or worse than another. Only over time do differences in benefits and harms become clearly visible.

> Just as a side remark, here are two examples of policy differences that I find curious and interesting:

> In the European Union, retail eggs are usually not washed, and rules say they must not be chilled below 5 °C before sale, to avoid condensation that can help bacteria pass through the shell. That keeps the egg's natural cuticle, the "bloom", intact.  
> In the United States, shell eggs are typically washed and sanitized. That removes most of the cuticle, so regulations require a continuous cold chain at or below 7.2 °C soon after the egg is laid and through retail.  
> The cuticle is a physical and chemical barrier that helps block bacterial entry. Washing weakens or removes it. The United States counterbalances that with sanitizing and refrigeration.
> Salmonella can be on the shell, and it can be inside the egg from infection of the hen's reproductive tract. Washing and refrigerating help avoid one path of infection.
> A continuous cold chain does use more energy.
> Risk and energy trade offs depend on where in the chain cooling happens and for how long.

> Germany follows the Franco German "stay and play" model for emergency medical response. A physician, Notarzt, can be dispatched to the scene to start advanced treatment before transport, often in a separate car that meets the ambulance, a "rendezvous system". That usually means more care on scene and transport once the patient is stabilized.  
> The United States follows the Anglo American "scoop and run" model for emergency medical response. Care is led by paramedics and EMTs, with advanced life support available, but the overall emphasis, especially for major trauma, is to get to the hospital quickly after essential interventions.  
> Because physicians often treat on scene in Germany, prehospital times can be longer than in systems that prioritize rapid transport.
> For cardiac arrest, many systems now favor high quality resuscitation on scene before moving, rather than "scoop and run" during chest compressions, to keep blood and oxygen flowing when the heart stops or breathing fails.

This weak coupling is not a conceptual problem for my outlined end to end scientific process, but a practical one.
The probability distributions for predictions will be broad and spread out, and only over long periods will they become clearly separable.

## Holistic Structural Realism (Model-Based, Predictivist)

This view of knowledge and science could be called **holistic structural realism**:
A view of knowledge and science that makes a minimal realist commitment and then judges theories strictly by how well their model-based structures predict the world.
It begins with an explicit "hinge" or "jump": from within pure thought one cannot prove that a mind-independent world exists, yet one adopts that world as a starting point in order to do inquiry at all.
This commitment is kept thin and revisable; it is not a proof but a practical premise that licenses measurement, modeling, and testing.

On this stance, a theory is best understood as a family of models rather than a mere list of sentences.
Inquiry works by carving **concepts** from observation and supplying **operations** that "link" (verknüpfen) those *concepts* so new ones can be generated - much as an algebra supplies *operations* over elements.
A scientist or agent translates a slice of observable reality into a model state (the *digital-twin*), lets the theory's *operations* run to derive consequences, and then translates those consequences back into predicted observables.
If, after some time, reality occupies the state the model forecast, the theory earns credit; if not, the theory is revised or replaced.

"Realism" here means that successful models are taken to latch onto the **structure** of the world - its relations, symmetries, and invariants.
What survives theory change and underwrites reliable prediction is the preserved structure and the *operations* that track it.
Truth is therefore understood as **approximate structural correspondence**: not infallible mirroring but increasing grip on the patterns that make accurate forecasting and control possible.

Judgment is thoroughly **predictivist**.
The currency of appraisal is out-of-sample performance, calibration, and error discovered under stress - rather than binary accept/reject rituals.
Thresholds (such as a p-value or a detection cutoff) are treated as downstream engineering choices tied to costs and risks, not as boundaries in the theory itself;
whenever possible, continuous scoring and robustness checks replace hard cutoffs.
Because perception and inference are fallible, improvement comes from iteration: models are conjectured, exposed to severe tests, and updated.
Evolutionary considerations explain why creatures begin with roughly truth-tracking heuristics, and communal methods (sharing evidence, criticizing, and updating - often in Bayesian form) explain how those heuristics are disciplined and extended.

Metaphysically, the view is intentionally **modest**.
It brackets undecidable debates that do not change model performance, retaining only the hinge needed to start inquiry and the working assumption that stable worldly structure is there to be learned.
Methodologically, it favors explicit mappings between data and model, transparent measurement procedures, and a readiness to retool *concepts* and *operations* when prediction fails or when new instruments reveal previously unseen structure.

This stance treats sustained predictive success as evidence of contact with how the world is structured, not merely as convenient fiction.
It candidly acknowledges the initial hinge while showing how that modest step suffices to launch effective science.
In contrast to views that make meaning wholly contingent on discourse or power, it holds that models answer to an external order that pushes back[^realityhurts] and sorts good *operations* from bad.

In short, holistic structural realism says: adopt the smallest realist premise that lets inquiry begin;
build theories as operation-rich models that encode worldly structure;
test them by their predictive fruits;
keep whatever keeps working;
and treat progress as tightening the structural fit between our models and the mind-independent patterns that make those successes possible.

### The Meaning of Holistic

I want to say a few words about what *holistic* means in *holistic structural realism*.
In many philosophical traditions, metaphysics (what reality is like) and its subcategory ontology (what exists) are treated separately from epistemology (how we know and what justifies belief).
In my outline of *The Normal Metaphysics and Epistemology: Holistic Structural Realism*, I do not see how the two areas can be kept separate.
Here, holism means that there is no sharp boundary and that our total theory, which means science + logic + philosophy, co-evolves as a web.

### Engineering Your Hinges

As said earlier, I avoid spending even one microsecond thinking about undecidable problems, as I consider it a complete waste of time.
That is why I think of the pre-jump territory as *engineering your hinges*: you pick the small set that minimizes regret and maximizes learning speed.
You can't prove your hinges true from nowhere, but you can score them by how they perform once adopted.
The goal is the fastest ramp into testable work and to adopt just enough to get learning off the ground.

A minimal, "shortest path into science" hinge-pack is:

* Existence & Regularity: A mind-independent world with some stable patterns.
* Fallible Reliability: Perception/memory are noisy but informative.
* Induction/Abduction: Past patterns offer defeasible guides; prefer models that explain more with less.
* Probability as the grammar of uncertainty: Update credences with data; pick loss functions before peeking.
* Error-correction norms: Replication, transparency, model checking, out-of-sample tests.
* Revisability: Any commitment can be revised if it predictably hurts prediction/explanation.

That way you keep the pre-jump thin, but you've bought everything you need to run the scientific method.

## Contra the Primacy of Language

I have forgotten where I heard the saying that women relate to other women face to face, while men relate to other men shoulder to shoulder.
I take it to mean that men bond side by side when working together, for example at a workbench on a piece of wood.
Working together is a task that requires a lot of coordination. Words are used, but most of it happens intuitively.

> Just as a side remark:
> The psychoanalyst [Elliott Jaques](https://en.wikipedia.org/wiki/Elliott_Jaques) coined the terms Engaged Direct Awareness and Disengaged Conscious Languaging in the book [The Life and Behavior of Living Organisms: A General Theory](https://www.amazon.com/Life-Behavior-Living-Organisms-General/dp/0275975010) to describe two different activities: in the moment coordination, and conscious thinking in the mind.  
> Engaged Direct Awareness (EDA) is the immediate, embodied, nonverbal “felt” awareness while acting in the world.  
> Disengaged Conscious Languaging (DCL) is a reflective, language based, distancing mode that names, represents, and reasons about experience.


### History of Computation, Logic and Algebra

[Ron Pressler](https://inside.java/u/RonPressler), known for Project Loom, which brought virtual threads to the Java programming language, has written a three part blog series called [Finite of Sense and Infinite of Thought: A History of Computation, Logic and Algebra](https://pron.github.io/computation-logic-algebra).
He shows that computation, logic, and algebra were studied as one subject for most of their history, related in the following way: human reasoning works by computation, logic describes the process of reasoning, and algebra is the mathematical model of logic.
The three began to separate into different disciplines only towards the end of the nineteenth century and the beginning of the twentieth.
Logic was separated from algebra by [Gottlob Frege](https://en.wikipedia.org/wiki/Gottlob_Frege).
Some decades later, [Alan Turing](https://en.wikipedia.org/wiki/Alan_Turing) separated computation from logic by treating it as a more foundational *concept*.

In those posts he quotes [Leibniz](https://en.wikipedia.org/wiki/Gottfried_Wilhelm_Leibniz):

> When God calculates and exercises his thought, the world is made.

He goes on to say: "This emphasis on language cannot be overstated, and it would come to dominate all work in logic in the subsequent centuries, at least until Turing".

Pressler shows how logic was conceived as mechanized reasoning through language: we manipulate symbols whose meaning ties them to reality.
Frege separated logic from algebra and built a constructed ideography (Begriffsschrift) to reduce the ambiguity of natural language.
Turing, in a non linguistic turn, analyzed computation itself as step by step mechanical *operations* that do not depend on any particular language or formalism.
This separated computation from logic and made computation the primitive substrate that comes before semantics.

I argue that *concepts* live at the level of *computation* and do not depend on any particular language or formalism.

### Pre-linguistic Nature of Concepts

It happens regularly to me that I have a gut feeling about a new *concept*, but I lack the words to describe it.
I then search for books that might point in that direction.
Often I buy several books and start reading the first hundred pages or so to see if the author is talking about the *concept* I have in mind.
If I am lucky, at least one of those books describes that *concept* and *gives me* the language to talk about it.
I admit that as I gain language, the *concepts* become much clearer in my mind.
These authors have often spent much time with the *concept* and describe extensions and variations that make it richer for me.
But this does not change the fact that the *concept* existed in a pre-linguistic form in my mind, and that I could use it even before I had a name for it.

The best case scenario, which makes me as happy as a child in front of a Christmas tree, is to find an "end-of-life book", written by a person who has spent a whole career on a topic and then, towards the end of that career, writes it down.

One example from my past was finding the *concept* of [Human Capability](https://www.amazon.in/Human-Capability-Individual-Potential-Evaluation/dp/0962107077) through the article [Elliott Jaques Levels With You](https://www.strategy-business.com/article/10938) by [Art Kleiner](https://www.artkleiner.com/about).
The article also mentiones the idea of *time span of discretion*, which is the longest period over which a person can be held personally accountable to deliver a specified result without further direction or review.

I said earlier that I only noticed how different people *really* are once I became a manager responsible for other people's work.
Then, I also noticed this *concept* of human *capability*, which in my view is orthogonal to intelligence as measured by IQ.
Capability relates to a planning horizon, while IQ is about mental processing in the here and now.
A doctor is an example of someone who often has a high IQ.
A doctor has to study a great deal to understand how the human body works and to bring many facts about a patient into context, but patient contacts are usually brief or occur over short periods.
On the other hand, a farmer also needs to know many things, but a key part of a farmer's work is control and mastery of the yearly cycle.
This brief introduction must suffice and is meant only to add some color to the abstract descriptions above.

### The Role of Language

Coming again back to what I repeated now several times earlier that people are different even at the zeroth order approximation, as Galton's experiments showed.
This understanding suggests that the role language plays in different people's thinking process may differ, as also the internet meme of [Wordcel vs. Rotator](https://roonscape.ai/p/a-song-of-shapes-and-words) suggests.

The book [Document Engineering](https://www.amazon.com/Document-Engineering-Analyzing-Designing-Informatics/dp/0262072610) defines the term Document Assembly Model (DAM).
Think of the Document Assembly Model (DAM) as the rulebook that turns a rich, graph-like in-memory model into one concrete document tree: you pick a root, follow specific associations ("a particular route through the network") to create a hierarchy.
That hierarchy is precisely what lets you serialize the model into a linear document, since assembly models are designed for preparing the conceptual graph for encoding.

In a similar way, I see language as the underlying "technology" for serializing and encoding the graph of thoughts into a linear form that can be communicated to other people.

> As a side note:
> In [Conflict free Replicated Data Types](https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type) (CRDTs) there are two styles of replication: state based (CvRDT), where replicas periodically share the entire state of the data structure and merge it, and operation based (CmRDT), where replicas broadcast each update and others apply the same updates so everyone converges to the same state.  
> For a machine the two styles are interchangeable and more a matter of taste. I believe that communication between humans works only in an operation based way.
> This is also suggested by the idea that good communicators meet their audience where they are and build from there.

In that role language is very important, and the act of putting thoughts onto paper usually refines and clarifies them.
This sentiment is echoed in [Specifying Systems](https://www.amazon.com/Specifying-Systems-Language-Hardware-Engineers/dp/032114306X), where the author [Leslie Lamport](https://en.wikipedia.org/wiki/Leslie_Lamport) says:

> Writing is nature's way of letting you know how sloppy your thinking is.  
> Mathematics is nature's way of letting you know how sloppy your writing is.  
> Formal mathematics is nature's way of letting you know how sloppy your mathematics is.

Another line that fits here is the saying, "My pencil and I are more clever than I am", which is often attributed to Einstein.
All of this underlines the importance of language in the thinking process.
Nevertheless, I argue that those effects are additions to the raw thinking process in *concepts*, which is pre-linguistic and does not require language.


## Additivity of Jumps: Religion, Free Will, ...

I am active in, and feel close to, the [skeptics](https://en.wikipedia.org/wiki/The_Skeptics_Society) community.
Many skeptics are quite hostile to religious positions and show a sense of superiority, although in my view there is no basis[^omphalos] for it.

My position begins by candidly acknowledging a logical gap between the pre-jump world and the world in which one can begin to apply the scientific method.
I know I cannot prove from nowhere the hinges I have engineered.
Because of that, there is legitimate room for further jumps, such as religion or free will.

### Christianity

I do not know much about religions other than Christianity, but I would say that their line of argument is not bad and may even be considered quite good.

In the theological tradition, faith and knowledge are distinguished.
Knowledge is what we can recognize with our own natural reasoning.
By contrast, faith means accepting as true, on someone else's word, things we cannot know for ourselves.
This kind of faith runs through our whole life, even outside religion.
When we need information in a field we do not understand, we go to an expert.
If needed, we check that the person really is an expert and has the right qualifications.
We then trust what they tell us for a rational reason: they are a specialist and know what they are talking about.
This is also how evidence is judged in court, where eyewitnesses are questioned and experts are heard.
So the *concept* of faith has a rational basis.

It is the same with religious faith.
By our natural reason we can recognize very little about God, so here too we depend on experts.
We must therefore decide on rational grounds which experts we will believe.
Jesus Christ came claiming to reveal to us the full truth about God.
The rational reason to believe him is that he performed extraordinary miracles and, above all, that he rose from the dead.
We did not see this ourselves, so we must also take it on faith.
The rational reason to do so is that we have the reports of witnesses who "ate and drank with him after he rose from the dead" (Acts 10:41) and who were willing to die for this testimony.
That makes their testimony credible.

A Christian is therefore not an agnostic.
Through rationally grounded faith, a Christian has knowledge about God that God has revealed to people.

As Pope Benedict XVI said: Nothing that you come to know through reason contradicts what you believed by faith.
Put another way: Faith and science, each in its own domain of competence, offer mutual support… Truth cannot contradict truth.

Faith and science are compatible because the first jump described above is compatible with further jumps, such as belief in God or belief that humans have free will.

## Footnotes

[^mentalmasturbation]: See also [Philosophy is mental masturbation](https://www.reddit.com/r/philosophy/comments/fy4as/philosophy_is_mental_masturbation).
[^stevenreissnormal]: The American psychologist [Steven Reiss](https://en.wikipedia.org/wiki/Steven_Reiss) objected to letting diagnostic thinking dominate our picture of human nature, and he wrote [*The Normal Personality: A New Way of Thinking about People*](https://www.amazon.com/Normal-Personality-Thinking-About-People/dp/0521707447) to build personality theory from the ground up, starting with the motivational building blocks of everyday, mentally healthy people.
[^andrickbegriffshandwerker]: [Michael Andrick](https://derandrick.de), a doctor of philosophy, has described the profession of a philosopher as a *Begriffshandwerker* (a craftsman of *concepts*).
[^daviddeutschtranscript]: You can find the transscript of that conversation [here](https://sanalabs.com/strange-loop/david-deutsch).
[^realityhurts]: Several idioms point in this direction. One is, "Reality is what hurts if you act against it". Another is a variation on "Wisdom is the fear of God", rendered as "Wisdom is the fear of the truth". You better be afraid of the truth, because it will hit you in the face if you are wrong.
[^omphalos]: Creationists could adopt a pure "outside-the-system" stance - the modern form of [Philip Gosse](https://en.wikipedia.org/wiki/Philip_Henry_Gosse)'s 1857 [Omphalos](https://en.wikipedia.org/wiki/Omphalos_(book)) idea: God created a fully functioning cosmos only a few thousand years ago but with every photon, fossil and zircon already in place, so empirical dating is irrelevant. That move is logically watertight (unfalsifiable) but it is not the path most organised young-Earth ministries take today. In principle you could also swap out Gosse's "God planted fossils" for "God (or a coder-God) spun up a high-resolution simulation ten thousand years ago with a 13.8-billion-year back-story pre-rendered", which leaves Gosse's original line of argument basically unchanged.